{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70wfh6FzDPbl"
      },
      "source": [
        "# Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rg5Gbw-DNpHK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import torch\n",
        "from transformers import DistilBertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import random\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtTc9lvtDciw"
      },
      "source": [
        "# Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Wj52v_3bDdk7",
        "outputId": "53962d11-72f8-4044-d0e9-ea3764ef1c62"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Chen Kaige followed up the Unprecedented succe...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Promoted by Wakefield Poole with an advertisin...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the Jean Renoir film \"La Grande Illusion\", an ...</td>\n",
              "      <td>yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sing Your Song shows not only Harry Belafonte'...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What makes Jennifer Connelly so Remarkable isn...</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence Class\n",
              "0  Chen Kaige followed up the Unprecedented succe...   yes\n",
              "1  Promoted by Wakefield Poole with an advertisin...   yes\n",
              "2  the Jean Renoir film \"La Grande Illusion\", an ...   yes\n",
              "3  Sing Your Song shows not only Harry Belafonte'...    no\n",
              "4  What makes Jennifer Connelly so Remarkable isn...    no"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load the dataset\n",
        "data = pd.read_excel(r\"C:\\Users\\alish\\OneDrive\\Documents\\Alishbah\\DASC5309_DATA SCIENCE CAPSTONE PROJECT\\dataset\\classification.xlsx\")\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRBYgvCYOEPm"
      },
      "source": [
        "# DistilBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\alish\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, Dataset\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define your custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "# Convert class labels to integers\n",
        "label_to_id = {'yes': 1, 'no': 0}\n",
        "data['Label'] = data['Class'].map(label_to_id)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.15, random_state=42)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the training and testing data\n",
        "train_encodings = tokenizer(list(train_data['Sentence']), truncation=True, padding=True, return_tensors=\"pt\")\n",
        "train_labels = torch.tensor(train_data['Label'].values)\n",
        "\n",
        "test_encodings = tokenizer(list(test_data['Sentence']), truncation=True, padding=True, return_tensors=\"pt\")\n",
        "test_labels = torch.tensor(test_data['Label'].values)\n",
        "\n",
        "# Convert to torch datasets\n",
        "train_dataset = TextDataset(train_encodings, train_labels)\n",
        "test_dataset = TextDataset(test_encodings, test_labels)\n",
        "\n",
        "# Define ranges for hyperparameters\n",
        "learning_rates = [1e-5, 2e-5, 3e-5]\n",
        "batch_sizes = [16, 32, 64]\n",
        "num_epochs_options = [3, 5, 10]\n",
        "\n",
        "# Initialize a list to store the results\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for batch_size in batch_sizes:\n",
        "        for num_epochs in num_epochs_options:\n",
        "            # Initialize the model\n",
        "            model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
        "            model.to(device)\n",
        "\n",
        "            # Initialize optimizer\n",
        "            optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "            # Define KFold Cross-Validation\n",
        "            kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "            # Initialize metrics\n",
        "            fold_metrics = {\n",
        "                'accuracy': [],\n",
        "                'precision': [],\n",
        "                'recall': [],\n",
        "                'f1_score': []\n",
        "            }\n",
        "\n",
        "            for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "                # Data loaders for the current fold\n",
        "                train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_ids))\n",
        "                val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_ids))\n",
        "\n",
        "                # Training loop\n",
        "                for epoch in range(num_epochs):\n",
        "                    model.train()\n",
        "                    for batch in train_loader:\n",
        "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                        outputs = model(**batch)\n",
        "                        loss = outputs.loss\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Evaluation loop\n",
        "                model.eval()\n",
        "                predictions = []\n",
        "                real_values = []\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader:\n",
        "                        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                        outputs = model(**batch)\n",
        "                        logits = outputs.logits\n",
        "                        predictions.extend(torch.argmax(logits, dim=1).tolist())\n",
        "                        real_values.extend(batch['labels'].tolist())\n",
        "\n",
        "                # Calculate metrics\n",
        "                accuracy = accuracy_score(real_values, predictions)\n",
        "                precision = precision_score(real_values, predictions, average='weighted')\n",
        "                recall = recall_score(real_values, predictions, average='weighted')\n",
        "                f1 = f1_score(real_values, predictions, average='weighted')\n",
        "\n",
        "                fold_metrics['accuracy'].append(accuracy)\n",
        "                fold_metrics['precision'].append(precision)\n",
        "                fold_metrics['recall'].append(recall)\n",
        "                fold_metrics['f1_score'].append(f1)\n",
        "\n",
        "            # Calculate average metrics across folds\n",
        "            avg_metrics = {metric: sum(values) / len(values) for metric, values in fold_metrics.items()}\n",
        "\n",
        "            # Store the results\n",
        "            result = {\n",
        "                'learning_rate': lr,\n",
        "                'batch_size': batch_size,\n",
        "                'num_epochs': num_epochs,\n",
        "                'avg_accuracy': avg_metrics['accuracy'],\n",
        "                'avg_precision': avg_metrics['precision'],\n",
        "                'avg_recall': avg_metrics['recall'],\n",
        "                'avg_f1_score': avg_metrics['f1_score']\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Print the results\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvYbj1rxhJMF",
        "outputId": "dc417947-da9d-4ae5-f8b1-893cb1b66aca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-11-25 18:06:07,325] A new study created in memory with name: no-name-dfa1211c-61d9-4556-abbf-04b72d4eb879\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-25 19:18:10,728] Trial 0 finished with value: 0.5291666666666667 and parameters: {'lr': 0.0003192813554104076, 'batch_size': 16, 'num_epochs': 10}. Best is trial 0 with value: 0.5291666666666667.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-25 19:54:55,699] Trial 1 finished with value: 0.5291666666666667 and parameters: {'lr': 0.0003474134591721711, 'batch_size': 16, 'num_epochs': 5}. Best is trial 0 with value: 0.5291666666666667.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-25 20:17:16,058] Trial 2 finished with value: 0.9305272108843538 and parameters: {'lr': 7.864253497053805e-05, 'batch_size': 16, 'num_epochs': 3}. Best is trial 2 with value: 0.9305272108843538.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-25 21:26:36,797] Trial 3 finished with value: 0.9510204081632653 and parameters: {'lr': 7.853125121945287e-05, 'batch_size': 16, 'num_epochs': 10}. Best is trial 3 with value: 0.9510204081632653.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-25 22:00:42,994] Trial 4 finished with value: 0.8222789115646257 and parameters: {'lr': 0.00024109919336653057, 'batch_size': 16, 'num_epochs': 5}. Best is trial 3 with value: 0.9510204081632653.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-25 23:09:26,424] Trial 5 finished with value: 0.963265306122449 and parameters: {'lr': 9.251970632779431e-05, 'batch_size': 16, 'num_epochs': 10}. Best is trial 5 with value: 0.963265306122449.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-26 00:23:35,237] Trial 6 finished with value: 0.9673469387755101 and parameters: {'lr': 0.00010462147123907267, 'batch_size': 16, 'num_epochs': 10}. Best is trial 6 with value: 0.9673469387755101.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-26 01:35:43,515] Trial 7 finished with value: 0.4340986394557823 and parameters: {'lr': 0.0006371015716636106, 'batch_size': 32, 'num_epochs': 10}. Best is trial 6 with value: 0.9673469387755101.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-26 02:10:46,135] Trial 8 finished with value: 0.6428571428571429 and parameters: {'lr': 0.0005324138846070539, 'batch_size': 32, 'num_epochs': 5}. Best is trial 6 with value: 0.9673469387755101.\n",
            "C:\\Users\\alish\\AppData\\Local\\Temp\\ipykernel_17316\\2182678417.py:39: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[I 2023-11-26 02:30:24,570] Trial 9 finished with value: 0.5863095238095238 and parameters: {'lr': 0.0005531976952982788, 'batch_size': 32, 'num_epochs': 3}. Best is trial 6 with value: 0.9673469387755101.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of finished trials: 10\n",
            "Best trial: {'lr': 0.00010462147123907267, 'batch_size': 16, 'num_epochs': 10}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, Dataset\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import optuna\n",
        "import pandas as pd\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Custom dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Define the objective function for Optuna\n",
        "def objective(trial):\n",
        "    # Hyperparameters to tune\n",
        "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-3)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "    num_epochs = trial.suggest_categorical('num_epochs', [3, 5, 10])\n",
        "\n",
        "    # Prepare data\n",
        "    label_to_id = {'yes': 1, 'no': 0}\n",
        "    data['Label'] = data['Class'].map(label_to_id)\n",
        "    train_data, test_data = train_test_split(data, test_size=0.15, random_state=42)\n",
        "\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "    train_encodings = tokenizer(list(train_data['Sentence']), truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    train_labels = torch.tensor(train_data['Label'].values)\n",
        "\n",
        "    train_dataset = TextDataset(train_encodings, train_labels)\n",
        "\n",
        "    # KFold Cross-Validation\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    accuracy_list = []\n",
        "\n",
        "    # Set the seed and device\n",
        "    set_seed()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_to_id))\n",
        "    model.to(device)\n",
        "\n",
        "    for fold, (train_ids, val_ids) in enumerate(kfold.split(train_dataset)):\n",
        "        # Data loaders for the current fold\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(train_ids))\n",
        "        val_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=SubsetRandomSampler(val_ids))\n",
        "\n",
        "        # Model and optimizer\n",
        "        optimizer = AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            for batch in train_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "                loss = outputs.loss\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Evaluation\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        real_values = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(**batch)\n",
        "                predictions.extend(torch.argmax(outputs.logits, dim=1).tolist())\n",
        "                real_values.extend(batch['labels'].tolist())\n",
        "\n",
        "        # Calculate accuracy for the current fold\n",
        "        accuracy = accuracy_score(real_values, predictions)\n",
        "        accuracy_list.append(accuracy)\n",
        "\n",
        "    # Return the average accuracy over the folds\n",
        "    return np.mean(accuracy_list)\n",
        "\n",
        "# Create an Optuna study and optimize\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=10)  # Adjust the number of trials as necessary\n",
        "\n",
        "# Print the results\n",
        "print('Number of finished trials:', len(study.trials))\n",
        "print('Best trial:', study.best_trial.params)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
